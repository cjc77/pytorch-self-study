{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dafbc06-b177-43b6-b62d-b3293b4a7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import unicodedata\n",
    "import string\n",
    "import os\n",
    "import time\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29da3248-c1cc-4955-bf31-89f433ef885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    \"\"\"Unicode string to plain ASCII.\n",
    "    \n",
    "    \"\"\"\n",
    "    decoded = \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "        and c in all_letters\n",
    "    )\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def read_lines(filename):\n",
    "    \"\"\"Read a file and split into lines.\n",
    "    \n",
    "    \"\"\"\n",
    "    lines = open(filename, encoding=\"utf-8\").read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "\n",
    "def random_ex(train=True):\n",
    "    \"\"\"Select a random training example and return category/line contents.\n",
    "    \n",
    "    \"\"\"\n",
    "    category = np.random.choice(all_categories)\n",
    "    # 0 = train, 1 = val\n",
    "    if train:\n",
    "        line_ps = (1 - category_lines_tv_split[category]) / (1 - category_lines_tv_split[category]).sum()\n",
    "    else:\n",
    "        line_ps = category_lines_tv_split[category] / category_lines_tv_split[category].sum()\n",
    "    \n",
    "    line = np.random.choice(category_lines[category], p=line_ps)\n",
    "    return category, line\n",
    "\n",
    "\n",
    "def category_tensor(category):\n",
    "    \"\"\"Get 1-hot encoding for a category \n",
    "    \n",
    "    \"\"\"\n",
    "    cat_i = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][cat_i] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc33aee-11bb-4c9e-b0ba-53213b83780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_files = glob.glob(\"../data/rnn_char_class_data/names/*.txt\")\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "# +1 is for EOS marker\n",
    "n_letters = len(all_letters) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2acca8-bd89-42b5-9eef-e681525c81d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_lines dict. -- a list of lines per category\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "for filename in name_files:\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = read_lines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "category_lines_tv_split = {k: np.random.choice((0, 1), p=(0.8, 0.2), size=len(v)) for k, v in category_lines.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41677579-fb41-43a9-ba78-e768e1b63952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_lines_tv_split[\"Korean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881aa316-2d11-45cd-aa78-5752c757a458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categories: 18 ['Korean', 'Vietnamese', 'Czech', 'French', 'Greek', 'Dutch', 'Irish', 'Japanese', 'Portuguese', 'Polish', 'Spanish', 'Scottish', 'German', 'Russian', 'English', 'Arabic', 'Chinese', 'Italian']\n",
      "O'Neal\n"
     ]
    }
   ],
   "source": [
    "print('# categories:', n_categories, all_categories)\n",
    "print(unicode_to_ascii(\"O'Néàl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be4eb39-cdcb-4641-b4f8-e88622def31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, category_size, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.h1_fc = nn.Linear(category_size + input_size + hidden_size, hidden_size)\n",
    "        self.y1_fc = nn.Linear(category_size + input_size + hidden_size, output_size)\n",
    "        self.y2_fc = nn.Linear(hidden_size + output_size, output_size)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "                \n",
    "    def forward(self, c, x, h0):\n",
    "        cxh0 = torch.cat((c, x, h0), dim=1)\n",
    "        \n",
    "        h1 = self.h1_fc(cxh0)\n",
    "        h1 = self.tanh(h1)\n",
    "        \n",
    "        y = self.y1_fc(cxh0)\n",
    "        y = self.tanh(y)\n",
    "        \n",
    "        yh1 = torch.cat((y, h1), dim=1)\n",
    "        y = self.y2_fc(yh1)\n",
    "        y = self.log_softmax(y)\n",
    "        \n",
    "        return y, h1\n",
    "    \n",
    "    def zero_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35b3ec76-539c-4af2-ac12-1e2a51dd8216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('English', 'Cliff')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_ex(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c03d7275-f7f2-42e3-961f-b340435a47ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_tensor(\"Japanese\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
