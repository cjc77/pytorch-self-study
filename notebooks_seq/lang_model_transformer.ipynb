{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1abdce-57f8-4702-84c4-3e9ba6e76d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tempfile import TemporaryDirectory\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1452315e-7381-422e-9865-bd38f7123118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter):\n",
    "    \"\"\"Convert raw text into a flat Tensor.\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        torch.tensor(vocab(tokenizer(item)), dtype=torch.long)\n",
    "        for item in raw_text_iter\n",
    "    ]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "\n",
    "def batchify(data, bsz, device):\n",
    "    \"\"\"Divides data into 'bsz' separate sequences & removes extra elements\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generate upper-triangular matrix of ``-inf`` with zeros on the diagonal.\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float(\"-inf\"), diagonal=1)\n",
    "\n",
    "\n",
    "def get_batch(x_src, i, bptt=35):\n",
    "    \"\"\"\n",
    "    x_src is a tensor of shape (full_seq_len, batch_size).\n",
    "    \n",
    "    Returns a tuple (data, target) where data has shape (seq_len, batch_size) and\n",
    "    target has shape (seq_len * batch_size)\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(x_src) - 1 - i)\n",
    "    data = x_src[i: i + seq_len]\n",
    "    target = x_src[(i + 1): (i + 1 + seq_len)].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def train(train_data, model, optimizer, criterion, bptt, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    \n",
    "    num_batches = len(train_data) // bptt\n",
    "    # Set size - 1 as upper bound since targets come from a look-ahead\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        seq_len = data.size(0)\n",
    "        # If last batch \n",
    "        if seq_len != bptt:\n",
    "            src_mask = src_mask[:seq_len, :seq_len]\n",
    "        yhat = model(data, src_mask)\n",
    "        loss = criterion(yhat.view(-1, n_tokens), targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = np.exp(cur_loss)\n",
    "            print((\n",
    "                f\"| epoch {epoch} | batch {batch}/{num_batches} \"\n",
    "                f\"| lr {np.round(lr, 5)} | ms/batch {np.round(ms_per_batch, 2)} \"\n",
    "                f\"| loss {np.round(cur_loss, 2)} | ppl {np.round(ppl, 2)}\"\n",
    "            ))\n",
    "            total_loss = 0.0\n",
    "            start_time = time.time()\n",
    "            \n",
    "\n",
    "def evaluate(eval_data, model, optimizer, criterion, bptt, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        # Set size - 1 as upper bound since targets come from a look-ahead\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            yhat = model(data, src_mask)\n",
    "            yhat_flat = yhat.view(-1, n_tokens)\n",
    "            total_loss += seq_len * criterion(yhat_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b34fd7d-1a19-4aa0-994e-5442a2ab8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_tokens, d_model, n_heads, d_hid, n_layers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.encoder = nn.Embedding(n_tokens, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, n_heads, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, n_layers)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, n_tokens)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "\n",
    "    def forward(self, x_src, x_src_mask):\n",
    "        x_src = self.encoder(x_src) * np.sqrt(self.d_model)\n",
    "        x_src = self.pos_encoder(x_src)\n",
    "        x_dest = self.transformer_encoder(x_src, x_src_mask)\n",
    "        x_dest = self.decoder(x_dest)\n",
    "        return x_dest\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        # Odd positions\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        # Even positions\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is a tensor of shape (seq_len, batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "417f8e5f-a276-4f4a-a649-aecb5b3cb7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(split=\"train\")\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38086558-002f-4f37-b69f-7d9ebeba3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57abb00-7058-48bf-b91f-ed54a650c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "chkpt_path = \"../checkpoints/lang_model_transformer.pth\"\n",
    "batch_size = 20\n",
    "eval_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d4ba98c-c03a-4656-a599-b86a6f543840",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = batchify(train_data, batch_size, device)\n",
    "val_data = batchify(val_data, eval_batch_size, device)\n",
    "test_data = batchify(test_data, eval_batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "343d266b-1b63-43f8-9e89-fe868409908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of vocab\n",
    "n_tokens = len(vocab)\n",
    "# Embedding dimension\n",
    "emsize = 200\n",
    "# Dimension size of feed forward network in TransformerEncoder\n",
    "d_hid = 200\n",
    "# Number of TransformerEncoderLayer in TransformerEncoder\n",
    "n_layers = 2\n",
    "# Number of heads in MultiheadAttention\n",
    "n_head = 2\n",
    "# Dropout probability (used by all network modules)\n",
    "dropout = 0.2\n",
    "\n",
    "bptt = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f367e0-b866-42be-94ec-63a676efe1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(n_tokens, emsize, n_head, d_hid, n_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6225bb00-bcc7-467f-885d-bc9244db959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786f1bba-ea6e-443c-88cf-314e4eeafd01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 | batch 200/2928 | lr [5.] | ms/batch 15.85 | loss 8.22 | ppl 3717.18\n",
      "| epoch 1 | batch 400/2928 | lr [5.] | ms/batch 11.78 | loss 6.88 | ppl 974.82\n",
      "| epoch 1 | batch 600/2928 | lr [5.] | ms/batch 11.1 | loss 6.45 | ppl 630.48\n",
      "| epoch 1 | batch 800/2928 | lr [5.] | ms/batch 10.61 | loss 6.31 | ppl 547.87\n",
      "| epoch 1 | batch 1000/2928 | lr [5.] | ms/batch 11.51 | loss 6.19 | ppl 486.05\n",
      "| epoch 1 | batch 1200/2928 | lr [5.] | ms/batch 10.85 | loss 6.16 | ppl 471.79\n",
      "| epoch 1 | batch 1400/2928 | lr [5.] | ms/batch 10.69 | loss 6.11 | ppl 452.55\n",
      "| epoch 1 | batch 1600/2928 | lr [5.] | ms/batch 10.76 | loss 6.11 | ppl 449.5\n",
      "| epoch 1 | batch 1800/2928 | lr [5.] | ms/batch 10.87 | loss 6.02 | ppl 411.21\n",
      "| epoch 1 | batch 2000/2928 | lr [5.] | ms/batch 11.45 | loss 6.02 | ppl 410.79\n",
      "| epoch 1 | batch 2200/2928 | lr [5.] | ms/batch 10.96 | loss 5.89 | ppl 361.48\n",
      "| epoch 1 | batch 2400/2928 | lr [5.] | ms/batch 11.13 | loss 5.97 | ppl 392.38\n",
      "| epoch 1 | batch 2600/2928 | lr [5.] | ms/batch 11.49 | loss 5.95 | ppl 382.6\n",
      "| epoch 1 | batch 2800/2928 | lr [5.] | ms/batch 11.46 | loss 5.87 | ppl 353.76\n",
      "-----------------------------------------------------------\n",
      "| end of epoch 1 | time 35.98 | valid loss 5.76 | valid ppl 317.96\n",
      "-----------------------------------------------------------\n",
      "| epoch 2 | batch 200/2928 | lr [4.75] | ms/batch 11.11 | loss 5.86 | ppl 350.43\n",
      "| epoch 2 | batch 400/2928 | lr [4.75] | ms/batch 10.88 | loss 5.84 | ppl 344.27\n",
      "| epoch 2 | batch 600/2928 | lr [4.75] | ms/batch 10.64 | loss 5.66 | ppl 286.8\n",
      "| epoch 2 | batch 800/2928 | lr [4.75] | ms/batch 10.82 | loss 5.69 | ppl 297.31\n",
      "| epoch 2 | batch 1000/2928 | lr [4.75] | ms/batch 11.17 | loss 5.64 | ppl 280.66\n",
      "| epoch 2 | batch 1200/2928 | lr [4.75] | ms/batch 10.72 | loss 5.68 | ppl 291.82\n",
      "| epoch 2 | batch 1400/2928 | lr [4.75] | ms/batch 10.74 | loss 5.68 | ppl 292.77\n",
      "| epoch 2 | batch 1600/2928 | lr [4.75] | ms/batch 10.63 | loss 5.7 | ppl 299.88\n",
      "| epoch 2 | batch 1800/2928 | lr [4.75] | ms/batch 10.66 | loss 5.64 | ppl 282.12\n",
      "| epoch 2 | batch 2000/2928 | lr [4.75] | ms/batch 11.01 | loss 5.65 | ppl 285.55\n",
      "| epoch 2 | batch 2200/2928 | lr [4.75] | ms/batch 10.72 | loss 5.54 | ppl 254.33\n",
      "| epoch 2 | batch 2400/2928 | lr [4.75] | ms/batch 10.67 | loss 5.64 | ppl 280.28\n",
      "| epoch 2 | batch 2600/2928 | lr [4.75] | ms/batch 10.72 | loss 5.64 | ppl 280.76\n",
      "| epoch 2 | batch 2800/2928 | lr [4.75] | ms/batch 10.78 | loss 5.57 | ppl 262.07\n",
      "-----------------------------------------------------------\n",
      "| end of epoch 2 | time 33.72 | valid loss 5.68 | valid ppl 293.9\n",
      "-----------------------------------------------------------\n",
      "| epoch 3 | batch 200/2928 | lr [4.5125] | ms/batch 10.76 | loss 5.59 | ppl 267.45\n",
      "| epoch 3 | batch 400/2928 | lr [4.5125] | ms/batch 10.67 | loss 5.6 | ppl 270.58\n",
      "| epoch 3 | batch 600/2928 | lr [4.5125] | ms/batch 10.8 | loss 5.41 | ppl 224.57\n",
      "| epoch 3 | batch 800/2928 | lr [4.5125] | ms/batch 11.23 | loss 5.47 | ppl 237.8\n",
      "| epoch 3 | batch 1000/2928 | lr [4.5125] | ms/batch 10.73 | loss 5.43 | ppl 227.06\n",
      "| epoch 3 | batch 1200/2928 | lr [4.5125] | ms/batch 10.83 | loss 5.47 | ppl 237.6\n",
      "| epoch 3 | batch 1400/2928 | lr [4.5125] | ms/batch 10.77 | loss 5.48 | ppl 239.24\n",
      "| epoch 3 | batch 1600/2928 | lr [4.5125] | ms/batch 10.82 | loss 5.51 | ppl 247.0\n",
      "| epoch 3 | batch 1800/2928 | lr [4.5125] | ms/batch 10.72 | loss 5.46 | ppl 234.7\n",
      "| epoch 3 | batch 2000/2928 | lr [4.5125] | ms/batch 10.96 | loss 5.47 | ppl 236.37\n",
      "| epoch 3 | batch 2200/2928 | lr [4.5125] | ms/batch 11.35 | loss 5.34 | ppl 208.44\n",
      "| epoch 3 | batch 2400/2928 | lr [4.5125] | ms/batch 12.23 | loss 5.44 | ppl 230.7\n",
      "| epoch 3 | batch 2600/2928 | lr [4.5125] | ms/batch 11.05 | loss 5.45 | ppl 233.42\n",
      "| epoch 3 | batch 2800/2928 | lr [4.5125] | ms/batch 12.14 | loss 5.39 | ppl 219.46\n",
      "-----------------------------------------------------------\n",
      "| end of epoch 3 | time 34.77 | valid loss 5.61 | valid ppl 274.07\n",
      "-----------------------------------------------------------\n",
      "| epoch 4 | batch 200/2928 | lr [4.28688] | ms/batch 11.75 | loss 5.42 | ppl 226.76\n",
      "| epoch 4 | batch 400/2928 | lr [4.28688] | ms/batch 11.34 | loss 5.44 | ppl 230.96\n",
      "| epoch 4 | batch 600/2928 | lr [4.28688] | ms/batch 11.15 | loss 5.25 | ppl 190.02\n",
      "| epoch 4 | batch 800/2928 | lr [4.28688] | ms/batch 10.81 | loss 5.32 | ppl 204.73\n",
      "| epoch 4 | batch 1000/2928 | lr [4.28688] | ms/batch 11.14 | loss 5.27 | ppl 194.1\n",
      "| epoch 4 | batch 1200/2928 | lr [4.28688] | ms/batch 12.0 | loss 5.31 | ppl 202.65\n",
      "| epoch 4 | batch 1400/2928 | lr [4.28688] | ms/batch 11.28 | loss 5.34 | ppl 208.3\n",
      "| epoch 4 | batch 1600/2928 | lr [4.28688] | ms/batch 10.77 | loss 5.37 | ppl 213.89\n",
      "| epoch 4 | batch 1800/2928 | lr [4.28688] | ms/batch 11.99 | loss 5.31 | ppl 203.13\n",
      "| epoch 4 | batch 2000/2928 | lr [4.28688] | ms/batch 11.87 | loss 5.32 | ppl 204.51\n",
      "| epoch 4 | batch 2200/2928 | lr [4.28688] | ms/batch 11.89 | loss 5.2 | ppl 180.66\n",
      "| epoch 4 | batch 2400/2928 | lr [4.28688] | ms/batch 11.24 | loss 5.31 | ppl 202.8\n",
      "| epoch 4 | batch 2600/2928 | lr [4.28688] | ms/batch 11.16 | loss 5.32 | ppl 204.08\n",
      "| epoch 4 | batch 2800/2928 | lr [4.28688] | ms/batch 11.4 | loss 5.25 | ppl 190.42\n",
      "-----------------------------------------------------------\n",
      "| end of epoch 4 | time 35.43 | valid loss 5.58 | valid ppl 265.57\n",
      "-----------------------------------------------------------\n",
      "| epoch 5 | batch 200/2928 | lr [4.07253] | ms/batch 11.38 | loss 5.29 | ppl 198.43\n",
      "| epoch 5 | batch 400/2928 | lr [4.07253] | ms/batch 11.0 | loss 5.32 | ppl 203.45\n",
      "| epoch 5 | batch 600/2928 | lr [4.07253] | ms/batch 10.93 | loss 5.14 | ppl 170.03\n",
      "| epoch 5 | batch 800/2928 | lr [4.07253] | ms/batch 11.08 | loss 5.2 | ppl 181.03\n",
      "| epoch 5 | batch 1000/2928 | lr [4.07253] | ms/batch 11.19 | loss 5.15 | ppl 172.82\n",
      "| epoch 5 | batch 1200/2928 | lr [4.07253] | ms/batch 11.05 | loss 5.19 | ppl 179.88\n",
      "| epoch 5 | batch 1400/2928 | lr [4.07253] | ms/batch 11.45 | loss 5.22 | ppl 184.6\n",
      "| epoch 5 | batch 1600/2928 | lr [4.07253] | ms/batch 11.12 | loss 5.25 | ppl 191.21\n",
      "| epoch 5 | batch 1800/2928 | lr [4.07253] | ms/batch 10.91 | loss 5.2 | ppl 181.8\n",
      "| epoch 5 | batch 2000/2928 | lr [4.07253] | ms/batch 10.77 | loss 5.22 | ppl 184.97\n",
      "| epoch 5 | batch 2200/2928 | lr [4.07253] | ms/batch 10.81 | loss 5.1 | ppl 163.64\n",
      "| epoch 5 | batch 2400/2928 | lr [4.07253] | ms/batch 10.89 | loss 5.2 | ppl 182.05\n",
      "| epoch 5 | batch 2600/2928 | lr [4.07253] | ms/batch 11.8 | loss 5.21 | ppl 183.4\n",
      "| epoch 5 | batch 2800/2928 | lr [4.07253] | ms/batch 11.62 | loss 5.15 | ppl 172.78\n",
      "-----------------------------------------------------------\n",
      "| end of epoch 5 | time 34.93 | valid loss 5.59 | valid ppl 266.69\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_data, model, optimizer, criterion, bptt, device)\n",
    "    val_loss = evaluate(val_data, model, optimizer, criterion, bptt, device)\n",
    "    val_ppl = np.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    \n",
    "    print(\"-\" * 59)\n",
    "    print((\n",
    "        f\"| end of epoch {epoch} | time {np.round(elapsed, 2)} \"\n",
    "        f\"| valid loss {np.round(val_loss, 2)} | valid ppl {np.round(val_ppl, 2)}\"\n",
    "    ))\n",
    "    print(\"-\" * 59)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    \n",
    "        checkpoint = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"criterion\": criterion\n",
    "        }\n",
    "        torch.save(checkpoint, chkpt_path)\n",
    "    scheduler.step()\n",
    "    \n",
    "checkpoint = torch.load(chkpt_path)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "criterion = checkpoint[\"criterion\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bead90b6-a4c7-4c58-8ec4-630cc3637bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| End of training | test loss 5.49 test ppl 242.57\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(test_data, model, optimizer, criterion, bptt, device)\n",
    "test_ppl = np.exp(test_loss)\n",
    "print(\"-\" * 59)\n",
    "print((\n",
    "    f\"| End of training | test loss {np.round(test_loss, 2)} \"\n",
    "    f\"test ppl {np.round(test_ppl, 2)}\"\n",
    "))\n",
    "print(\"-\" * 59)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
