{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88de0b6-ff8a-47da-a4cc-05ced90bb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from io import open\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95c5639-875c-483c-bcec-a2b32c7f6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ff3ef8-774d-4349-a2d6-d9117edf062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4637bfb-6199-40af-a9ae-b2e48c421531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 2\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "class EncoderS2S(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, h0):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        x = embedded\n",
    "        x, h = self.gru(x, h0)\n",
    "        return x, h\n",
    "    \n",
    "    def zero_hidden(self, device):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "# TODO: try training a simple seq-to-seq model\n",
    "class DecoderS2S(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Input & output size are the same (since they are word embeddings)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, h0):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        x = F.relu(embedded)\n",
    "        x, h = self.gru(x, h0)\n",
    "        # GRU output has leading batch dimension, so index x[0]\n",
    "        x = self.out(x[0])\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def zero_hidden(self, device):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "    \n",
    "class AttnDecoderS2S(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, max_length, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        # Weights for each word in sentence of max length\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(p = self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, x, h0, yhats_encoder):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        x = torch.concat((embedded[0], h0[0]), dim=1)\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(x),\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        # unsqueeze(0) --> add 'batch' dimension back to\n",
    "        # attention weights & add to encoder outputs\n",
    "        attn_applied = torch.bmm(\n",
    "            attn_weights.unsqueeze(0),\n",
    "            yhats_encoder.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        # subscript embedded & attn_applied to emove 'batch' dimension\n",
    "        x = torch.concat((embedded[0], attn_applied[0]), dim=1)\n",
    "        # unsqueeze result to add 'batch' dimension back in\n",
    "        x = self.attn_combine(x).unsqueeze(0)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x, h = self.gru(x, h0)\n",
    "        # GRU output has leading batch dimension, so index x[0]\n",
    "        x = self.out(x[0])\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x, h, attn_weights\n",
    "    \n",
    "    def zero_hidden(self, device):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f4db68-2408-4c29-9076-79307be3ab71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    \"\"\"Turn a Unicode string to plain ASCII.\n",
    "    \"\"\"\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_string(s):\n",
    "    \"\"\"Lowercase, trim, and remove non-letter characters.\n",
    "    \"\"\"\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def read_langs(lang1, lang2, data_path=\"../data/rnn_seq_to_seq_data\", reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    # Read file & split into lines\n",
    "    lines = open(f\"{data_path}/{lang1}-{lang2}.txt\", encoding=\"utf-8\").read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def filter_pair(p, max_length, eng_prefixes):\n",
    "    return len(p[0].split(\" \")) < max_length and \\\n",
    "        len(p[1].split(\" \")) < max_length and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filter_pairs(pairs, max_length, eng_prefixes):\n",
    "    return [pair for pair in pairs if filter_pair(pair, max_length, eng_prefixes)]\n",
    "\n",
    "\n",
    "def prepare_data(lang1, lang2, max_length, eng_prefixes, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1, lang2, reverse=reverse)\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = filter_pairs(pairs, max_length, eng_prefixes)\n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "\n",
    "def tensor_from_sentence(lang, sentence, device):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensors_from_pairs(pair, input_lang, output_lang, device):\n",
    "    input_tensor = tensor_from_sentence(input_lang, pair[0], device)\n",
    "    target_tensor = tensor_from_sentence(output_lang, pair[1], device)\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = int(np.round(s // 60))\n",
    "    s = int(np.round(s % 60))\n",
    "    return f\"{m}m {s}s\"\n",
    "\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / percent\n",
    "    rs = es - s\n",
    "    return f\"{as_minutes(s)} (- {as_minutes(rs)})\"\n",
    "\n",
    "\n",
    "def train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, input_tensor, target_tensor, max_length, device, teacher_forcing_ratio=0.5):\n",
    "    h_encoder = encoder.zero_hidden(device)\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.shape[0]\n",
    "    target_length = target_tensor.shape[0]\n",
    "    \n",
    "    yhats_encoder = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "    loss = 0.0\n",
    "    for ei in range(input_length):\n",
    "        yhat_encoder, h_encoder = encoder(input_tensor[ei], h_encoder)\n",
    "        yhats_encoder[ei] = yhat_encoder[0, 0]\n",
    "        \n",
    "    x_decoder = torch.tensor([[SOS_token]], device=device)\n",
    "    h_decoder = h_encoder\n",
    "    \n",
    "    use_teacher_forcing = True if np.random.uniform() < teacher_forcing_ratio else False\n",
    "    \n",
    "    # Teacher forcing = feed target in as next input\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            # Pass in full encoder output\n",
    "            yhat_decoder, h_decoder, attn_decoder = decoder(x_decoder, h_decoder, yhats_encoder)\n",
    "            loss += criterion(yhat_decoder, target_tensor[di])\n",
    "            \n",
    "            x_decoder = target_tensor[di]\n",
    "    # Non-teacher forcing = feed previous output of decoder as input\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            yhat_decoder, h_decoder, attn_decoder = decoder(x_decoder, h_decoder, yhats_encoder) \n",
    "            loss += criterion(yhat_decoder, target_tensor[di])\n",
    "            \n",
    "            topv, topi = yhat_decoder.topk(1)\n",
    "            # When used as input, should be detached from backprop history\n",
    "            x_decoder = topi.squeeze().detach()\n",
    "            if x_decoder.item() == EOS_token:\n",
    "                break\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "def train_iters(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, n_iters, input_lang, output_lang, max_length, device, teacher_forcing_ratio=0.5, print_every=1000, plot_every=100):\n",
    "    start_time = time.time()\n",
    "    plot_losses = []\n",
    "    # Reset by print_every\n",
    "    print_loss_total = 0.0\n",
    "    # Reset by plot_every\n",
    "    plot_loss_total = 0.0\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    training_pairs = [tensors_from_pairs(random.choice(pairs), input_lang, output_lang, device) for _ in range(n_iters)]\n",
    "    for itr in range(1, n_iters + 1):\n",
    "        input_tensor, target_tensor = training_pairs[itr - 1]\n",
    "        \n",
    "        loss = train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, input_tensor, target_tensor, max_length, device, teacher_forcing_ratio)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if itr % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0.0\n",
    "            print(f\"{time_since(start_time, itr / n_iters)} ({itr}, {np.round(itr / n_iters * 100, 1)}) {np.round(print_loss_avg, 3)}\")\n",
    "        \n",
    "        if itr % plot_every == 0:\n",
    "            # TODO\n",
    "            pass\n",
    "\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang, max_length, device):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensor_from_sentence(input_lang, sentence, device)\n",
    "        input_length = input_tensor.shape[0]\n",
    "        h_encoder = encoder.zero_hidden(device)\n",
    "        \n",
    "        yhats_encoder = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            yhat_encoder, h_encoder = encoder(input_tensor[ei], h_encoder)\n",
    "            yhats_encoder[ei] += yhat_encoder[0, 0]\n",
    "            \n",
    "        x_decoder = torch.tensor([[SOS_token]], device=device)\n",
    "        h_decoder = h_encoder\n",
    "        \n",
    "        decoded_words = []\n",
    "        attns_decoder = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        # TODO: implement beam search\n",
    "        for di in range(max_length):\n",
    "            yhat_decoder, h_decoder, attn_decoder = decoder(x_decoder, h_decoder, yhats_encoder)\n",
    "            attns_decoder[di] = attn_decoder.detach()\n",
    "            \n",
    "            topv, topi = yhat_decoder.detach().topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "                \n",
    "            x_decoder = topi.squeeze().detach()\n",
    "            \n",
    "        return decoded_words, attns_decoder[:di + 1]\n",
    "\n",
    "    \n",
    "def evaluate_beam(encoder, decoder, sentence, input_lang, output_lang, max_length, device, k=3, n=1):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get encoder output\n",
    "        input_tensor = tensor_from_sentence(input_lang, sentence, device)\n",
    "        input_length = input_tensor.shape[0]\n",
    "        h_encoder = encoder.zero_hidden(device)\n",
    "        \n",
    "        yhats_encoder = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            yhat_encoder, h_encoder = encoder(input_tensor[ei], h_encoder)\n",
    "            yhats_encoder[ei] += yhat_encoder[0, 0]\n",
    "        \n",
    "        x_decoder = torch.tensor([[SOS_token]], device=device)\n",
    "        h_decoder = h_encoder\n",
    "    \n",
    "    # Store (output so far, next input, hidden state, attention, length, score)\n",
    "    top_k = []\n",
    "    # Get initial candidates\n",
    "    with torch.no_grad():\n",
    "        attns_decoder = torch.zeros(max_length, max_length)\n",
    "        yhat_decoder, h_decoder, attn_decoder = decoder(x_decoder, h_decoder, yhats_encoder)\n",
    "        attns_decoder[0] = attn_decoder.detach()\n",
    "        \n",
    "        topv, topi = yhat_decoder.squeeze().detach().topk(k)\n",
    "        for i, ti in enumerate(topi):\n",
    "            decoded_words = []\n",
    "            if ti.item() == EOS_token:\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[ti.item()])\n",
    "            top_k.append(\n",
    "                (decoded_words, ti.squeeze().detach(), h_decoder, attns_decoder, 1, topv[i].item())\n",
    "            )\n",
    "    top_k = sorted(top_k, key=lambda x: x[-1] / x[-2], reverse=True)\n",
    "    # print(f\"State at step 1\")\n",
    "    # for item in top_k:\n",
    "    #     print(item[0])\n",
    "    \n",
    "    for di in range(1, max_length):\n",
    "        top_k_new = []\n",
    "        for decoded_words, x_decoder, h_decoder, attns_decoder, length, score in top_k:\n",
    "            if decoded_words[-1] == \"<EOS>\":\n",
    "                top_k_new.append(\n",
    "                    (decoded_words, x_decoder, h_decoder, attns_decoder, length, score)\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            yhat_decoder, h_decoder, attn_decoder = decoder(x_decoder, h_decoder, yhats_encoder)\n",
    "            attns_decoder[di] = attn_decoder.detach()\n",
    "            \n",
    "            topv, topi = yhat_decoder.squeeze().detach().topk(k)\n",
    "            for i, ti in enumerate(topi):\n",
    "                decoded_words_copy = deepcopy(decoded_words)\n",
    "                if ti.item() == EOS_token:\n",
    "                    decoded_words_copy.append(\"<EOS>\")\n",
    "                else:\n",
    "                    decoded_words_copy.append(output_lang.index2word[ti.item()])\n",
    "                top_k_new.append(\n",
    "                    (decoded_words_copy, ti.squeeze().detach(), h_decoder, torch.clone(attns_decoder), len(decoded_words_copy), score + topv[i].item())\n",
    "                )\n",
    "        top_k = sorted(\n",
    "            top_k_new,\n",
    "            key=lambda x: x[-1] / np.power(x[-2], 0.7),\n",
    "            reverse=True\n",
    "        )[:k]\n",
    "        # print(f\"State at step {di + 1}\")\n",
    "        # for item in top_k:\n",
    "        #     print(item[0])\n",
    "    return [(item[0], item[3][:len(item[0])]) for item in top_k[:n]]\n",
    "\n",
    "def evaluate_rand(encoder, decoder, pairs, input_lang, output_lang, max_length, device, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print(\"=>\", pair[0])\n",
    "        print(\"==\", pair[1])\n",
    "        yhats, attns = evaluate(encoder, decoder, pair[0], input_lang, output_lang, max_length, device)\n",
    "        output_sentence = ' '.join(yhats)\n",
    "        print(\"<=\", output_sentence)\n",
    "        print('')\n",
    "        \n",
    "        \n",
    "def show_attns(input_sentence, output_words, attns):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attns.numpy(), cmap=\"bone\")\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels(\n",
    "        [''] + input_sentence.split(' ') + [\"<EOS>\"],\n",
    "        rotation=90\n",
    "    )\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def evaluate_and_show_attns(encoder, decoder, input_sentence, input_lang, output_lang, max_length, device):\n",
    "    output_words, attns = evaluate(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        input_sentence,\n",
    "        input_lang,\n",
    "        output_lang,\n",
    "        max_length,\n",
    "        device\n",
    "    )\n",
    "    print(f\"Input: {input_sentence}\")\n",
    "    print(f\"Output: {' '.join(output_words)}\")\n",
    "    show_attns(input_sentence, output_words, attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba5106a-a252-4a57-8dbf-fd7f11c4c056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('a', 2), ('a', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([('a', 1), ('a', 3), ('a', 2)], key=lambda x: x[-1] * 3, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3806272d-3b50-436b-84f6-963aee3b2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25f0b5fa-2c2b-4fb2-9800-43d03d3bea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data(\"eng\", \"fra\", MAX_LENGTH, eng_prefixes, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d60027b6-ff97-426d-9dab-0f54101b036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "lr = 0.001\n",
    "n_iters = int(7e4)\n",
    "# Use previous trained model\n",
    "load_chkpt = False\n",
    "chkpt_path = \"../checkpoints/seq_to_seq_translation_attn.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f29e2304-d595-4dd4-8ced-dbd64d9d61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderS2S(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder = AttnDecoderS2S(output_lang.n_words, hidden_size, MAX_LENGTH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45cce860-ea44-4b3d-8836-729283402a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_optimizer = optim.SGD(encoder.parameters(), lr=lr)\n",
    "# decoder_optimizer = optim.SGD(attn_decoder.parameters(), lr=lr)\n",
    "encoder_optimizer = optim.AdamW(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.AdamW(attn_decoder.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194fd514-93b7-4e9c-9920-e6c7f27783c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 50s (- 40m 57s) (1400, 2.0) 2.952\n",
      "1m 25s (- 33m 59s) (2800, 4.0) 2.556\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(chkpt_path) and load_chkpt:\n",
    "    checkpoint = torch.load(chkpt_path)\n",
    "    encoder.load_state_dict(checkpoint[\"encoder_state_dict\"])\n",
    "    attn_decoder.load_state_dict(checkpoint[\"decoder_state_dict\"])\n",
    "    encoder_optimizer.load_state_dict(checkpoint[\"encoder_optimizer_state_dict\"])\n",
    "    decoder_optimizer.load_state_dict(checkpoint[\"decoder_optimizer_state_dict\"])\n",
    "    criterion = checkpoint[\"criterion\"]\n",
    "else:\n",
    "    train_iters(\n",
    "        encoder,\n",
    "        attn_decoder,\n",
    "        encoder_optimizer,\n",
    "        decoder_optimizer,\n",
    "        criterion,\n",
    "        n_iters,\n",
    "        input_lang,\n",
    "        output_lang,\n",
    "        MAX_LENGTH,\n",
    "        device,\n",
    "        print_every=int(n_iters/50)\n",
    "    )\n",
    "    checkpoint = {\n",
    "        \"encoder_state_dict\": encoder.state_dict(),\n",
    "        \"decoder_state_dict\": attn_decoder.state_dict(),\n",
    "        \"encoder_optimizer_state_dict\": encoder_optimizer.state_dict(),\n",
    "        \"decoder_optimizer_state_dict\": decoder_optimizer.state_dict(),\n",
    "        \"criterion\": criterion\n",
    "    }\n",
    "    torch.save(checkpoint, chkpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1a095-afc9-44e5-834c-9e414ead2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = pairs[1501]\n",
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe667a-da42-4cc9-a080-b48c23df99b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_rand(encoder, attn_decoder, pairs, input_lang, output_lang, MAX_LENGTH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06600e03-850a-4ff8-83ed-25d4f6e85c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attns(\n",
    "    encoder,\n",
    "    attn_decoder,\n",
    "    \"je suis trop froid .\",\n",
    "    # \"je suis detendu .\",\n",
    "    input_lang,\n",
    "    output_lang,\n",
    "    MAX_LENGTH,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd9a985-7baa-4346-88d8-9129a1bcad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = [\n",
    "    \"elle a cinq ans de moins que moi .\",\n",
    "    \"elle est trop petit .\",\n",
    "    \"je ne crains pas de mourir .\",\n",
    "    \"c est un jeune directeur plein de talent .\"\n",
    "]\n",
    "\n",
    "for inp_s in input_sentences:\n",
    "    evaluate_and_show_attns(\n",
    "        encoder,\n",
    "        attn_decoder,\n",
    "        inp_s,\n",
    "        input_lang,\n",
    "        output_lang,\n",
    "        MAX_LENGTH,\n",
    "        device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf9173-6fa3-4fdc-a555-5522753696f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
