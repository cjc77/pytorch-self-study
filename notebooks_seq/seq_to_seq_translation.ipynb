{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88de0b6-ff8a-47da-a4cc-05ced90bb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95c5639-875c-483c-bcec-a2b32c7f6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ff3ef8-774d-4349-a2d6-d9117edf062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4637bfb-6199-40af-a9ae-b2e48c421531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 2\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "class EncoderS2S(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, h0):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        x = embedded\n",
    "        x, h = self.gru(x, h0)\n",
    "        return x, h\n",
    "    \n",
    "    def zero_hidden(self, device):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "# TODO: try training a simple seq-to-seq model\n",
    "class DecoderS2S(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Input & output size are the same (since they are word embeddings)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, h0):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        x = F.relu(embedded)\n",
    "        x, h = self.gru(x, h0)\n",
    "        # GRU output has leading batch dimension, so index x[0]\n",
    "        x = self.out(x[0])\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def zero_hidden(self, device):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "    \n",
    "class AttnDecoderS2S(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, max_length, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        # Weights for each word in sentence of max length\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(p = self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, x, h0, yhats_encoder):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        x = torch.concat((embedded[0], h0[0]), dim=1)\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(x),\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        # unsqueeze(0) --> add 'batch' dimension back to\n",
    "        # attention weights & add to encoder outputs\n",
    "        attn_applied = torch.bmm(\n",
    "            attn_weights.unsqueeze(0),\n",
    "            yhats_encoder.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        # subscript embedded & attn_applied to emove 'batch' dimension\n",
    "        x = torch.concat((embedded[0], attn_applied[0]), dim=1)\n",
    "        # unsqueeze result to add 'batch' dimension back in\n",
    "        x = self.attn_combine(x).unsqueeze(0)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x, h = self.gru(x, h0)\n",
    "        # GRU output has leading batch dimension, so index x[0]\n",
    "        x = self.out(x[0])\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x, h, attn_weights\n",
    "    \n",
    "    def zero_hidden(self, device):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f4db68-2408-4c29-9076-79307be3ab71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    \"\"\"Turn a Unicode string to plain ASCII.\n",
    "    \"\"\"\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_string(s):\n",
    "    \"\"\"Lowercase, trim, and remove non-letter characters.\n",
    "    \"\"\"\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def read_langs(lang1, lang2, data_path=\"../data/rnn_seq_to_seq_data\", reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    # Read file & split into lines\n",
    "    lines = open(f\"{data_path}/{lang1}-{lang2}.txt\", encoding=\"utf-8\").read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def filter_pair(p, max_length, eng_prefixes):\n",
    "    return len(p[0].split(\" \")) < max_length and \\\n",
    "        len(p[1].split(\" \")) < max_length and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filter_pairs(pairs, max_length, eng_prefixes):\n",
    "    return [pair for pair in pairs if filter_pair(pair, max_length, eng_prefixes)]\n",
    "\n",
    "\n",
    "def prepare_data(lang1, lang2, max_length, eng_prefixes, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1, lang2, reverse=reverse)\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = filter_pairs(pairs, max_length, eng_prefixes)\n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "\n",
    "def tensor_from_sentence(lang, sentence, device):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensors_from_pairs(pair, input_lang, output_lang, device):\n",
    "    input_tensor = tensor_from_sentence(input_lang, pair[0], device)\n",
    "    target_tensor = tensor_from_sentence(output_lang, pair[1], device)\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = int(np.round(s // 60))\n",
    "    s = int(np.round(s % 60))\n",
    "    return f\"{m}m {s}s\"\n",
    "\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / percent\n",
    "    rs = es - s\n",
    "    return f\"{as_minutes(s)} (- {as_minutes(rs)})\"\n",
    "\n",
    "\n",
    "def train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, input_tensor, target_tensor, max_length, device, teacher_forcing_ratio=0.5):\n",
    "    h_encoder = encoder.zero_hidden(device)\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.shape[0]\n",
    "    target_length = target_tensor.shape[0]\n",
    "    \n",
    "    yhats_encoder = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "    loss = 0.0\n",
    "    for ei in range(input_length):\n",
    "        yhat_encoder, h_encoder = encoder(input_tensor[ei], h_encoder)\n",
    "        yhats_encoder[ei] = yhat_encoder[0, 0]\n",
    "        \n",
    "    x_decoder = torch.tensor([[SOS_token]], device=device)\n",
    "    h_decoder = h_encoder\n",
    "    \n",
    "    use_teacher_forcing = True if np.random.uniform() < teacher_forcing_ratio else False\n",
    "    \n",
    "    # Teacher forcing = feed target in as next input\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            # Pass in full encoder output\n",
    "            yhat_decoder, h_decoder, attn_decoder = decoder(x_decoder, h_decoder, yhats_encoder)\n",
    "            loss += criterion(yhat_decoder, target_tensor[di])\n",
    "            \n",
    "            x_decoder = target_tensor[di]\n",
    "    # Non-teacher forcing = feed previous output of decoder as input\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            yhat_decoder, h_decoder, attn_decoder = decoder(x_decoder, h_decoder, yhats_encoder) \n",
    "            loss += criterion(yhat_decoder, target_tensor[di])\n",
    "            \n",
    "            topv, topi = yhat_decoder.topk(1)\n",
    "            # When used as input, should be detached from backprop history\n",
    "            x_decoder = topi.squeeze().detach()\n",
    "            if x_decoder.item() == EOS_token:\n",
    "                break\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "def train_iters(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, n_iters, input_lang, output_lang, max_length, device, teacher_forcing_ratio=0.5, print_every=1000, plot_every=100):\n",
    "    start_time = time.time()\n",
    "    plot_losses = []\n",
    "    # Reset by print_every\n",
    "    print_loss_total = 0.0\n",
    "    # Reset by plot_every\n",
    "    plot_loss_total = 0.0\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    training_pairs = [tensors_from_pairs(random.choice(pairs), input_lang, output_lang, device) for _ in range(n_iters)]\n",
    "    for itr in range(1, n_iters + 1):\n",
    "        input_tensor, target_tensor = training_pairs[itr - 1]\n",
    "        \n",
    "        loss = train(encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, input_tensor, target_tensor, max_length, device, teacher_forcing_ratio)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if itr % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0.0\n",
    "            print(f\"{time_since(start_time, itr / n_iters)} ({itr}, {np.round(itr / n_iters * 100, 1)}) {np.round(print_loss_avg, 3)}\")\n",
    "        \n",
    "        if itr % plot_every == 0:\n",
    "            # TODO\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3806272d-3b50-436b-84f6-963aee3b2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f0b5fa-2c2b-4fb2-9800-43d03d3bea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data(\"eng\", \"fra\", MAX_LENGTH, eng_prefixes, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60027b6-ff97-426d-9dab-0f54101b036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "lr = 0.001\n",
    "n_iters = int(3e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f29e2304-d595-4dd4-8ced-dbd64d9d61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderS2S(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder = AttnDecoderS2S(output_lang.n_words, hidden_size, MAX_LENGTH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45cce860-ea44-4b3d-8836-729283402a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_optimizer = optim.SGD(encoder.parameters(), lr=lr)\n",
    "# decoder_optimizer = optim.SGD(attn_decoder.parameters(), lr=lr)\n",
    "encoder_optimizer = optim.AdamW(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.AdamW(attn_decoder.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194fd514-93b7-4e9c-9920-e6c7f27783c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 16s (- 15m 24s) (500, 1.7) 3.365\n",
      "0m 27s (- 12m 60s) (1000, 3.3) 2.92\n",
      "0m 38s (- 12m 7s) (1500, 5.0) 2.738\n",
      "0m 50s (- 11m 34s) (2000, 6.7) 2.65\n",
      "1m 1s (- 11m 10s) (2500, 8.3) 2.537\n",
      "1m 12s (- 10m 51s) (3000, 10.0) 2.432\n",
      "1m 24s (- 10m 34s) (3500, 11.7) 2.298\n",
      "1m 35s (- 10m 18s) (4000, 13.3) 2.193\n",
      "1m 47s (- 10m 5s) (4500, 15.0) 2.254\n",
      "1m 58s (- 9m 51s) (5000, 16.7) 2.087\n",
      "2m 9s (- 9m 36s) (5500, 18.3) 1.969\n",
      "2m 21s (- 9m 22s) (6000, 20.0) 2.013\n",
      "2m 33s (- 9m 12s) (6500, 21.7) 1.924\n",
      "2m 45s (- 9m 2s) (7000, 23.3) 1.862\n",
      "2m 57s (- 8m 51s) (7500, 25.0) 1.9\n",
      "3m 9s (- 8m 39s) (8000, 26.7) 1.703\n",
      "3m 20s (- 8m 27s) (8500, 28.3) 1.739\n",
      "3m 32s (- 8m 15s) (9000, 30.0) 1.768\n",
      "3m 44s (- 8m 4s) (9500, 31.7) 1.758\n",
      "3m 56s (- 7m 52s) (10000, 33.3) 1.595\n",
      "4m 8s (- 7m 41s) (10500, 35.0) 1.563\n",
      "4m 20s (- 7m 29s) (11000, 36.7) 1.495\n",
      "4m 32s (- 7m 18s) (11500, 38.3) 1.579\n",
      "4m 45s (- 7m 7s) (12000, 40.0) 1.524\n",
      "4m 57s (- 6m 56s) (12500, 41.7) 1.619\n",
      "5m 9s (- 6m 44s) (13000, 43.3) 1.471\n",
      "5m 21s (- 6m 32s) (13500, 45.0) 1.433\n",
      "5m 33s (- 6m 20s) (14000, 46.7) 1.415\n",
      "5m 45s (- 6m 9s) (14500, 48.3) 1.456\n",
      "5m 57s (- 5m 57s) (15000, 50.0) 1.362\n",
      "6m 9s (- 5m 45s) (15500, 51.7) 1.406\n",
      "6m 21s (- 5m 34s) (16000, 53.3) 1.34\n",
      "6m 33s (- 5m 22s) (16500, 55.0) 1.298\n",
      "6m 45s (- 5m 10s) (17000, 56.7) 1.285\n",
      "6m 57s (- 4m 58s) (17500, 58.3) 1.181\n",
      "7m 10s (- 4m 46s) (18000, 60.0) 1.233\n",
      "7m 22s (- 4m 35s) (18500, 61.7) 1.278\n",
      "7m 34s (- 4m 23s) (19000, 63.3) 1.232\n",
      "7m 46s (- 4m 11s) (19500, 65.0) 1.116\n",
      "7m 58s (- 3m 59s) (20000, 66.7) 1.243\n",
      "8m 10s (- 3m 47s) (20500, 68.3) 1.111\n",
      "8m 22s (- 3m 35s) (21000, 70.0) 1.16\n",
      "8m 35s (- 3m 23s) (21500, 71.7) 1.108\n",
      "8m 47s (- 3m 12s) (22000, 73.3) 1.202\n",
      "8m 59s (- 2m 60s) (22500, 75.0) 1.175\n",
      "9m 11s (- 2m 48s) (23000, 76.7) 1.055\n",
      "9m 23s (- 2m 36s) (23500, 78.3) 1.105\n",
      "9m 35s (- 2m 24s) (24000, 80.0) 1.126\n",
      "9m 48s (- 2m 12s) (24500, 81.7) 1.079\n",
      "9m 60s (- 1m 60s) (25000, 83.3) 1.067\n",
      "10m 11s (- 1m 48s) (25500, 85.0) 0.924\n",
      "10m 23s (- 1m 36s) (26000, 86.7) 1.099\n",
      "10m 36s (- 1m 24s) (26500, 88.3) 0.978\n",
      "10m 48s (- 1m 12s) (27000, 90.0) 1.047\n",
      "11m 0s (- 1m 0s) (27500, 91.7) 0.99\n",
      "11m 12s (- 0m 48s) (28000, 93.3) 1.053\n",
      "11m 25s (- 0m 36s) (28500, 95.0) 1.061\n",
      "11m 37s (- 0m 24s) (29000, 96.7) 0.984\n",
      "11m 49s (- 0m 12s) (29500, 98.3) 0.941\n",
      "12m 1s (- 0m 0s) (30000, 100.0) 1.06\n"
     ]
    }
   ],
   "source": [
    "train_iters(\n",
    "    encoder,\n",
    "    attn_decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    criterion,\n",
    "    n_iters,\n",
    "    input_lang,\n",
    "    output_lang,\n",
    "    MAX_LENGTH,\n",
    "    device,\n",
    "    print_every=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39493b39-81a8-4ae5-a13c-e2a286259daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
