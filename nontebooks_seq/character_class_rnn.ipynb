{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c67db5-2b5e-4295-af71-e8ce7bd15eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import unicodedata\n",
    "import string\n",
    "import os\n",
    "from io import open\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa83384-10f9-45f1-ba05-4519d4ee6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    \"\"\"Unicode string to plain ASCII.\n",
    "    \n",
    "    \"\"\"\n",
    "    decoded = \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "        and c in all_letters\n",
    "    )\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def read_lines(filename):\n",
    "    \"\"\"Read a file and split into lines.\n",
    "    \n",
    "    \"\"\"\n",
    "    lines = open(filename, encoding=\"utf-8\").read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "\n",
    "def letter_to_index(letter):\n",
    "    \"\"\"Find letter index from all_letters.\n",
    "    \n",
    "    \"\"\"\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def letter_to_tensor(letter):\n",
    "    \"\"\"Create 1-hot encoding of letters.\n",
    "    \n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letter_to_index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def line_to_tensor(line):\n",
    "    \"\"\"Convert line of text into (line, 1, n_letters) tensor.\n",
    "    \n",
    "    Extra dimension in middle is so a lines can be passed into\n",
    "    network 1-by-1 as a 2D vector of size (1, n_letters). \n",
    "    \n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for i, letter in enumerate(line):\n",
    "        tensor[i][0][letter_to_index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def cat_from_output(output):\n",
    "    \"\"\"Get category by finding argmax of output.\n",
    "    \n",
    "    \"\"\"\n",
    "    _, max_i = output.max(1)\n",
    "    category_i = max_i.item()\n",
    "    return all_categories[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "125c9157-90f4-4e40-90e9-c2c1ed5f8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.h_t1_fc = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.y_t1_fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, h_t0):\n",
    "        xh_t0 = torch.cat((x, h_t0), 1)\n",
    "        \n",
    "        h_t1 = self.h_t1_fc(xh_t0)\n",
    "        h_t1 = self.tanh(h_t1)\n",
    "        \n",
    "        y = self.y_t1_fc(h_t1)\n",
    "        y = self.log_softmax(y)\n",
    "        \n",
    "        return y, h_t1\n",
    "    \n",
    "    def zero_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d3dd58-8386-442e-9da1-14ba7e805503",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_files = glob.glob(\"../data/rnn_char_class_data/names/*.txt\")\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c84c49-66ce-4480-bb1d-b38b3260c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "print(unicode_to_ascii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918f43af-1dc5-40d9-b1bf-73eff235708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_lines dict. -- a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "for filename in name_files:\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = read_lines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3349e73-a1eb-4dbb-b599-648cdfff92a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115.2222222222222\n",
      "20074\n",
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    np.mean([len(v) for k, v in category_lines.items()])\n",
    ")\n",
    "\n",
    "print(\n",
    "    np.sum([len(v) for k, v in category_lines.items()])\n",
    ")\n",
    "\n",
    "print(category_lines[\"Italian\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c92dfb4-98f8-4d3f-9277-55682d1ca351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "print(letter_to_tensor(\"J\"))\n",
    "print(line_to_tensor(\"Jones\").size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c1a08c1-527b-4c7e-9a0a-05706526a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2da07894-2a56-433a-8f6f-4748ff8fb0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-character tensor\n",
    "with torch.no_grad():\n",
    "    x = letter_to_tensor(\"A\")\n",
    "    h_0 = rnn.zero_hidden()\n",
    "    y, h_1 = rnn(x, h_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce739909-b08f-446b-95f2-4b5aa8dff2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8451, -2.8543, -2.8373, -3.0047, -2.8773, -2.9277, -2.9962, -2.8439,\n",
      "         -2.9241, -2.8222, -2.8192, -2.8884, -2.9292, -2.9389, -2.9840, -2.8788,\n",
      "         -2.8788, -2.8085]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2046597-5101-4fff-bea2-5dcd5126e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full line tensor\n",
    "with torch.no_grad():\n",
    "    x = line_to_tensor(\"Albert\")\n",
    "    h_0 = rnn.zero_hidden()\n",
    "    y, h1 = rnn(x[0], h_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cecf2a04-08cc-497a-82fb-ff5c1cdcbe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8451, -2.8543, -2.8373, -3.0047, -2.8773, -2.9277, -2.9962, -2.8439,\n",
      "         -2.9241, -2.8222, -2.8192, -2.8884, -2.9292, -2.9389, -2.9840, -2.8788,\n",
      "         -2.8788, -2.8085]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "919171cc-cb07-4212-953d-8ecb59f84c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Italian', 17)\n"
     ]
    }
   ],
   "source": [
    "print(cat_from_output(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96afd822-f117-4071-bdf2-e082ad2b8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_ex():\n",
    "    category = np.random.choice(all_categories)\n",
    "    line = np.random.choice(category_lines[category])\n",
    "    category_tensor = torch.tensor(\n",
    "        [all_categories.index(category)],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    line_tensor = line_to_tensor(line)\n",
    "    return category, line, category_tensor, line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a080d72-5db6-4c53-8789-94905de77b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Korean',\n",
       " 'Ma',\n",
       " tensor([0]),\n",
       " tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_training_ex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9cd3294-160d-4d8e-a7f4-1a31fbf5d3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Vietnamese / Line: Vo\n",
      "Category: Korean / Line: Gu\n",
      "Category: Korean / Line: Chin\n",
      "Category: Korean / Line: Yun\n",
      "Category: Arabic / Line: Isa\n",
      "Category: Spanish / Line: Gutierrez\n",
      "Category: Czech / Line: Kenzel\n",
      "Category: Korean / Line: San\n",
      "Category: Scottish / Line: Wright\n",
      "Category: English / Line: Dunne\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = random_training_ex()\n",
    "    print(f\"Category: {category} / Line: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a71960a-17c6-4531-b03a-0966b01740ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
